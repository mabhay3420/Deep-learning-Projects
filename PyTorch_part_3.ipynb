{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch part 3",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTT7u0lywyPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,)),\n",
        "                              ])\n",
        "# Download and load the training data\n",
        "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EB-zkDNH1H_s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ebae509b-a45e-4963-eecc-b3939023b873"
      },
      "source": [
        "model = nn.Sequential(nn.Linear(784,128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128,64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(64,10),\n",
        "                      nn.LogSoftmax(dim=1) )\n",
        "criterion = nn.NLLLoss()\n",
        "# Get our data\n",
        "images, labels = next(iter(trainloader))\n",
        "# Flatten images\n",
        "images = images.view(images.shape[0], -1)\n",
        "logits = model(images)\n",
        "loss = criterion(logits,labels)\n",
        "print(loss)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.2866, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yQbR2yYGOU_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "7bcedec5-047b-4636-a651-9fadd65d17d0"
      },
      "source": [
        "model"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (3): ReLU()\n",
              "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
              "  (5): LogSoftmax()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFtjDxGp3w8I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "33ecb699-8c0b-4826-c5bd-365a4536c777"
      },
      "source": [
        "logits"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.3004, -2.1908, -2.2618, -2.2485, -2.3655, -2.2482, -2.4728, -2.0899,\n",
              "         -2.5155, -2.4095],\n",
              "        [-2.3113, -2.2104, -2.2233, -2.3082, -2.3080, -2.3073, -2.4139, -2.0780,\n",
              "         -2.4951, -2.4372],\n",
              "        [-2.3713, -2.2529, -2.2853, -2.2399, -2.2573, -2.2934, -2.3947, -2.0745,\n",
              "         -2.4787, -2.4406],\n",
              "        [-2.3453, -2.2432, -2.2815, -2.2732, -2.2895, -2.2846, -2.4395, -2.0752,\n",
              "         -2.4792, -2.3723],\n",
              "        [-2.3894, -2.1669, -2.3012, -2.2917, -2.2747, -2.2603, -2.4192, -2.1279,\n",
              "         -2.4548, -2.3920],\n",
              "        [-2.3428, -2.1467, -2.2957, -2.2988, -2.3041, -2.2811, -2.4029, -2.0692,\n",
              "         -2.4982, -2.4649],\n",
              "        [-2.3469, -2.2027, -2.2839, -2.3331, -2.3265, -2.2248, -2.4782, -2.0598,\n",
              "         -2.4346, -2.4055],\n",
              "        [-2.2973, -2.2276, -2.3156, -2.2684, -2.3351, -2.2342, -2.3560, -2.0892,\n",
              "         -2.4846, -2.4806],\n",
              "        [-2.4053, -2.1789, -2.2766, -2.2953, -2.2398, -2.3585, -2.4267, -2.0731,\n",
              "         -2.4401, -2.3979],\n",
              "        [-2.3601, -2.2149, -2.2913, -2.3079, -2.2847, -2.2539, -2.4496, -2.0860,\n",
              "         -2.4104, -2.4221],\n",
              "        [-2.3193, -2.2445, -2.2999, -2.2213, -2.2951, -2.2475, -2.4121, -2.1257,\n",
              "         -2.4940, -2.4196],\n",
              "        [-2.3156, -2.2438, -2.3727, -2.2619, -2.3118, -2.2233, -2.4013, -2.0657,\n",
              "         -2.4947, -2.4006],\n",
              "        [-2.3400, -2.2075, -2.2600, -2.3040, -2.2819, -2.2740, -2.3776, -2.0776,\n",
              "         -2.4770, -2.4950],\n",
              "        [-2.3153, -2.1871, -2.3166, -2.2616, -2.3048, -2.2620, -2.3898, -2.1457,\n",
              "         -2.4899, -2.3996],\n",
              "        [-2.2910, -2.2118, -2.2944, -2.2448, -2.3519, -2.2179, -2.4617, -2.0551,\n",
              "         -2.5146, -2.4737],\n",
              "        [-2.2893, -2.2165, -2.3138, -2.2887, -2.2976, -2.2644, -2.3903, -2.0718,\n",
              "         -2.5158, -2.4461],\n",
              "        [-2.3339, -2.2191, -2.3137, -2.2601, -2.3243, -2.2506, -2.3723, -2.1005,\n",
              "         -2.4791, -2.4249],\n",
              "        [-2.3593, -2.2296, -2.2785, -2.2534, -2.3347, -2.1903, -2.4157, -2.0925,\n",
              "         -2.4730, -2.4684],\n",
              "        [-2.3657, -2.1866, -2.3484, -2.2769, -2.2878, -2.2348, -2.4622, -2.0843,\n",
              "         -2.4383, -2.4051],\n",
              "        [-2.3718, -2.1938, -2.3403, -2.3049, -2.2497, -2.2361, -2.4308, -2.0739,\n",
              "         -2.4561, -2.4375],\n",
              "        [-2.3192, -2.2722, -2.3222, -2.2983, -2.2609, -2.2931, -2.3789, -2.0603,\n",
              "         -2.4848, -2.3919],\n",
              "        [-2.3399, -2.2130, -2.3382, -2.2467, -2.2690, -2.2748, -2.3626, -2.0780,\n",
              "         -2.4899, -2.4824],\n",
              "        [-2.4230, -2.2242, -2.2815, -2.3676, -2.2177, -2.3066, -2.3447, -2.1282,\n",
              "         -2.4067, -2.3659],\n",
              "        [-2.3626, -2.1764, -2.2866, -2.2759, -2.2694, -2.2893, -2.4247, -2.1110,\n",
              "         -2.4865, -2.4020],\n",
              "        [-2.3364, -2.1849, -2.3062, -2.2202, -2.3217, -2.2813, -2.3580, -2.1017,\n",
              "         -2.5116, -2.4729],\n",
              "        [-2.3279, -2.1756, -2.2997, -2.2692, -2.3033, -2.2539, -2.4401, -2.0988,\n",
              "         -2.4923, -2.4317],\n",
              "        [-2.3657, -2.2712, -2.3289, -2.2947, -2.2582, -2.2835, -2.4400, -2.0679,\n",
              "         -2.3881, -2.3768],\n",
              "        [-2.3564, -2.2355, -2.2742, -2.3041, -2.2549, -2.2410, -2.3741, -2.1131,\n",
              "         -2.4805, -2.4456],\n",
              "        [-2.3519, -2.1841, -2.3129, -2.2871, -2.3165, -2.2818, -2.3671, -2.1265,\n",
              "         -2.4122, -2.4265],\n",
              "        [-2.3516, -2.2799, -2.2432, -2.2318, -2.3652, -2.2591, -2.4007, -2.0836,\n",
              "         -2.4425, -2.4238],\n",
              "        [-2.3388, -2.2305, -2.2805, -2.2398, -2.3118, -2.2681, -2.4110, -2.1132,\n",
              "         -2.4530, -2.4285],\n",
              "        [-2.3132, -2.2563, -2.2248, -2.2406, -2.3618, -2.2236, -2.3861, -2.0936,\n",
              "         -2.5387, -2.4630],\n",
              "        [-2.3798, -2.1275, -2.2246, -2.2853, -2.3088, -2.3155, -2.4530, -2.0677,\n",
              "         -2.5097, -2.4470],\n",
              "        [-2.3442, -2.1511, -2.2148, -2.2632, -2.3999, -2.2417, -2.5131, -2.0422,\n",
              "         -2.5409, -2.4307],\n",
              "        [-2.3733, -2.2828, -2.2837, -2.2344, -2.2618, -2.2768, -2.4411, -2.0794,\n",
              "         -2.4494, -2.4003],\n",
              "        [-2.3663, -2.1874, -2.3644, -2.2371, -2.2959, -2.2564, -2.3540, -2.1485,\n",
              "         -2.4285, -2.4304],\n",
              "        [-2.3190, -2.1935, -2.2684, -2.2407, -2.3664, -2.2237, -2.4048, -2.1427,\n",
              "         -2.4910, -2.4332],\n",
              "        [-2.3292, -2.2311, -2.3445, -2.2312, -2.3142, -2.2646, -2.3871, -2.0699,\n",
              "         -2.4691, -2.4483],\n",
              "        [-2.3449, -2.2353, -2.2804, -2.2538, -2.3220, -2.2644, -2.3858, -2.0964,\n",
              "         -2.4411, -2.4535],\n",
              "        [-2.3158, -2.2203, -2.2800, -2.2368, -2.3105, -2.2670, -2.4572, -2.0868,\n",
              "         -2.4927, -2.4263],\n",
              "        [-2.3636, -2.2027, -2.2490, -2.3157, -2.2949, -2.2861, -2.4303, -2.1047,\n",
              "         -2.4321, -2.3966],\n",
              "        [-2.3583, -2.1567, -2.2371, -2.2529, -2.3697, -2.2810, -2.4833, -2.0442,\n",
              "         -2.5334, -2.4099],\n",
              "        [-2.3521, -2.1965, -2.3642, -2.3038, -2.2477, -2.2896, -2.3603, -2.1348,\n",
              "         -2.4250, -2.3897],\n",
              "        [-2.3552, -2.2601, -2.2066, -2.3157, -2.2572, -2.2486, -2.4126, -2.1007,\n",
              "         -2.5163, -2.4178],\n",
              "        [-2.3420, -2.2214, -2.4023, -2.2325, -2.2792, -2.2411, -2.3379, -2.0957,\n",
              "         -2.5069, -2.4317],\n",
              "        [-2.3158, -2.2184, -2.3172, -2.2687, -2.3077, -2.2577, -2.4452, -2.0927,\n",
              "         -2.4643, -2.3934],\n",
              "        [-2.3458, -2.2239, -2.2976, -2.2492, -2.3096, -2.2290, -2.4208, -2.1331,\n",
              "         -2.4418, -2.4215],\n",
              "        [-2.4058, -2.1793, -2.3180, -2.2899, -2.2394, -2.3076, -2.3866, -2.0867,\n",
              "         -2.4609, -2.4133],\n",
              "        [-2.3181, -2.2294, -2.3317, -2.2796, -2.2783, -2.2687, -2.4198, -2.0509,\n",
              "         -2.5002, -2.4197],\n",
              "        [-2.3745, -2.1422, -2.2963, -2.2549, -2.2902, -2.2615, -2.4032, -2.1588,\n",
              "         -2.4182, -2.4814],\n",
              "        [-2.3199, -2.2027, -2.2922, -2.2204, -2.3727, -2.2114, -2.4282, -2.0749,\n",
              "         -2.5181, -2.4716],\n",
              "        [-2.3459, -2.2653, -2.2754, -2.2977, -2.2553, -2.2883, -2.3768, -2.0763,\n",
              "         -2.4324, -2.4667],\n",
              "        [-2.3155, -2.1935, -2.2810, -2.3023, -2.3211, -2.2572, -2.4056, -2.0591,\n",
              "         -2.4855, -2.4809],\n",
              "        [-2.3266, -2.2161, -2.2619, -2.2779, -2.3149, -2.2790, -2.4842, -2.0676,\n",
              "         -2.4600, -2.4056],\n",
              "        [-2.3125, -2.2009, -2.2304, -2.2635, -2.3562, -2.2358, -2.4255, -2.0872,\n",
              "         -2.5335, -2.4619],\n",
              "        [-2.2993, -2.2281, -2.2485, -2.2682, -2.3299, -2.2642, -2.4171, -2.0784,\n",
              "         -2.4982, -2.4635],\n",
              "        [-2.3521, -2.1920, -2.3335, -2.2432, -2.2652, -2.2692, -2.4070, -2.0759,\n",
              "         -2.4885, -2.4734],\n",
              "        [-2.3220, -2.2436, -2.3181, -2.3276, -2.2919, -2.2765, -2.3726, -2.1039,\n",
              "         -2.4091, -2.3965],\n",
              "        [-2.3272, -2.1918, -2.2879, -2.2637, -2.3184, -2.2465, -2.4689, -2.0771,\n",
              "         -2.4860, -2.4319],\n",
              "        [-2.3244, -2.1648, -2.3191, -2.2442, -2.3193, -2.2491, -2.3750, -2.0932,\n",
              "         -2.4790, -2.5378],\n",
              "        [-2.3665, -2.2002, -2.3106, -2.2712, -2.2653, -2.2956, -2.3684, -2.1060,\n",
              "         -2.4944, -2.4010],\n",
              "        [-2.3647, -2.1919, -2.2842, -2.2463, -2.3473, -2.2485, -2.4209, -2.1009,\n",
              "         -2.4615, -2.4193],\n",
              "        [-2.3198, -2.2129, -2.2392, -2.2418, -2.4199, -2.1891, -2.4342, -2.0869,\n",
              "         -2.5103, -2.4584],\n",
              "        [-2.3958, -2.1862, -2.2555, -2.2666, -2.3091, -2.2365, -2.4564, -2.0875,\n",
              "         -2.4661, -2.4393]], grad_fn=<LogSoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjetqYyo990l",
        "colab_type": "text"
      },
      "source": [
        "## **The AutoGrad**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maAaYZOtwyPo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "81946bc3-563f-40ad-e1d8-90076b7412a3"
      },
      "source": [
        "x = torch.randn(2,2, requires_grad=True)\n",
        "print(x)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.4080, -0.2358],\n",
            "        [ 0.2004, -1.2097]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO77aY4IwyPs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "afa048d5-7c8d-4e4b-f6bd-cfb9ecda6fbf"
      },
      "source": [
        "y = x**2\n",
        "print(y)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.1665, 0.0556],\n",
            "        [0.0401, 1.4633]], grad_fn=<PowBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4nD-Gc1wyP4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "09acdaec-d9eb-4884-cc1d-bf135d5b714a"
      },
      "source": [
        "z = y.mean()\n",
        "print(z)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.4314, grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnvZovGBwyP9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "87091962-ce3f-4fe8-c159-3b8c121d6613"
      },
      "source": [
        "print(y.grad)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBaR23xhwyQD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "96b15024-d99c-4fb2-a7f0-9a8bd05b32e1"
      },
      "source": [
        "z.backward()\n",
        "\n",
        "print(x.grad)\n",
        "print(x/2)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.2040, -0.1179],\n",
            "        [ 0.1002, -0.6048]])\n",
            "tensor([[-0.2040, -0.1179],\n",
            "        [ 0.1002, -0.6048]], grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKJ5dm1nFtkg",
        "colab_type": "text"
      },
      "source": [
        "**loss and autograder together**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD_AK4xOFHKR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "58cd6925-f0fe-4e19-fe7c-2bf65fd17f19"
      },
      "source": [
        "print(\"before backward : \\n\",model[0].weight.grad)\n",
        "loss.backward()\n",
        "print(\"after backward : \\n\",model[0].weight.grad)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before backward : \n",
            " None\n",
            "after backward : \n",
            " tensor([[-9.7349e-05, -9.7349e-05, -9.7349e-05,  ..., -9.7349e-05,\n",
            "         -9.7349e-05, -9.7349e-05],\n",
            "        [ 1.1921e-03,  1.1921e-03,  1.1921e-03,  ...,  1.1921e-03,\n",
            "          1.1921e-03,  1.1921e-03],\n",
            "        [ 4.3366e-03,  4.3366e-03,  4.3366e-03,  ...,  4.3366e-03,\n",
            "          4.3366e-03,  4.3366e-03],\n",
            "        ...,\n",
            "        [ 2.2473e-03,  2.2473e-03,  2.2473e-03,  ...,  2.2473e-03,\n",
            "          2.2473e-03,  2.2473e-03],\n",
            "        [ 2.8787e-04,  2.8787e-04,  2.8787e-04,  ...,  2.8787e-04,\n",
            "          2.8787e-04,  2.8787e-04],\n",
            "        [-7.9090e-05, -7.9090e-05, -7.9090e-05,  ..., -7.9090e-05,\n",
            "         -7.9090e-05, -7.9090e-05]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5epncZIKHU6g",
        "colab_type": "text"
      },
      "source": [
        "**Optimization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBI4ljrxHERb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import optim\n",
        "optimizer = optim.SGD(model.parameters(),lr=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTUYW-KCwyQb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "6caa8c59-ba1d-4ce2-98d7-f09a775321d7"
      },
      "source": [
        "print('Initial weights - ', model[0].weight)\n",
        "\n",
        "images, labels = next(iter(trainloader))\n",
        "images.resize_(64, 784)\n",
        "\n",
        "# Clear the gradients, do this because gradients are accumulated\n",
        "optimizer.zero_grad()\n",
        "\n",
        "# Forward pass, then backward pass, then update weights\n",
        "output = model(images)\n",
        "loss = criterion(output, labels)\n",
        "loss.backward()\n",
        "print('Gradient -', model[0].weight.grad)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial weights -  Parameter containing:\n",
            "tensor([[ 0.0039, -0.0115,  0.0103,  ...,  0.0288,  0.0266, -0.0074],\n",
            "        [-0.0177,  0.0051,  0.0097,  ..., -0.0163, -0.0219, -0.0325],\n",
            "        [-0.0124,  0.0023, -0.0174,  ..., -0.0295, -0.0019, -0.0195],\n",
            "        ...,\n",
            "        [-0.0150, -0.0153,  0.0096,  ..., -0.0058, -0.0098,  0.0066],\n",
            "        [-0.0040,  0.0246,  0.0352,  ...,  0.0268,  0.0033,  0.0118],\n",
            "        [ 0.0237, -0.0038,  0.0240,  ..., -0.0350,  0.0350, -0.0193]],\n",
            "       requires_grad=True)\n",
            "Gradient - tensor([[-0.0010, -0.0010, -0.0010,  ..., -0.0010, -0.0010, -0.0010],\n",
            "        [-0.0015, -0.0015, -0.0015,  ..., -0.0015, -0.0015, -0.0015],\n",
            "        [ 0.0044,  0.0044,  0.0044,  ...,  0.0044,  0.0044,  0.0044],\n",
            "        ...,\n",
            "        [ 0.0017,  0.0017,  0.0017,  ...,  0.0017,  0.0017,  0.0017],\n",
            "        [ 0.0033,  0.0033,  0.0033,  ...,  0.0033,  0.0033,  0.0033],\n",
            "        [-0.0001, -0.0001, -0.0001,  ..., -0.0001, -0.0001, -0.0001]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmZoZtBywyQf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "ae9ad7aa-8307-4562-b474-df09f73fddd3"
      },
      "source": [
        "# Take an update step and few the new weights\n",
        "optimizer.step()\n",
        "print('Updated weights - ', model[0].weight)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated weights -  Parameter containing:\n",
            "tensor([[ 0.0039, -0.0115,  0.0103,  ...,  0.0288,  0.0266, -0.0074],\n",
            "        [-0.0177,  0.0051,  0.0097,  ..., -0.0163, -0.0219, -0.0325],\n",
            "        [-0.0124,  0.0022, -0.0174,  ..., -0.0296, -0.0019, -0.0195],\n",
            "        ...,\n",
            "        [-0.0150, -0.0153,  0.0096,  ..., -0.0058, -0.0098,  0.0066],\n",
            "        [-0.0040,  0.0245,  0.0351,  ...,  0.0268,  0.0033,  0.0117],\n",
            "        [ 0.0237, -0.0038,  0.0240,  ..., -0.0350,  0.0350, -0.0193]],\n",
            "       requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl5K-gqhwyQk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "54cae61a-abf8-4bf7-add8-cbe534a00933"
      },
      "source": [
        "\n",
        "\n",
        "model = nn.Sequential(nn.Linear(784, 128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128, 64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(64, 10),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
        "\n",
        "epochs = 5\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for images, labels in trainloader:\n",
        "        # Flatten MNIST images into a 784 long vector\n",
        "        images = images.view(images.shape[0], -1)\n",
        "    \n",
        "        # TODO: Training pass\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(images)\n",
        "        loss = criterion(output,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "    else:\n",
        "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss: 1.871593421964503\n",
            "Training loss: 0.8466294877755363\n",
            "Training loss: 0.5242825607691747\n",
            "Training loss: 0.42840172348818034\n",
            "Training loss: 0.38501152666265775\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf1vczGcwyQp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "87d0e55d-e5ab-49ea-94a9-c97c8cffac8d"
      },
      "source": [
        "%matplotlib inline\n",
        "import helper\n",
        "\n",
        "images, labels = next(iter(trainloader))\n",
        "\n",
        "img = images[0].view(1, 784)\n",
        "# Turn off gradients to speed up this part\n",
        "with torch.no_grad():\n",
        "    logps = model(img)\n",
        "\n",
        "# Output of the network are log-probabilities, need to take exponential for probabilities\n",
        "ps = torch.exp(logps)\n",
        "helper.view_classify(img.view(1, 28, 28), ps)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAADsCAYAAAAhDDIOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAU4UlEQVR4nO3de7BddX338feHAEauZkh0BBKDcimIQ6V5eMRWW8ulgA7Yqh2waFFGHgWtCPo81Np6acfBWnnap2IVARWrgFgvqFChgKKWoElArmIxBuQm4RZucknyff7YG2fP6dnJyXHts9YO79fMGfZZv7X3+pyThM/5rfU7e6WqkCSpazZpO4AkSZOxoCRJnWRBSZI6yYKSJHWSBSVJ6iQLSpLUSRaUpJFJ8oEk/9p2jg2VZGGSSrLpNJ9fSXYeMvZnSS6abN8kn0zy19NLvfGxoCT9RpK8PsmSJA8nuTPJhUl+r6UsleSRfpbbk5ySZFYbWYapqi9U1YFDxt5aVX8LkOQPktw2s+m6xYKSNG1JTgD+Efgw8BxgAfAJ4LAWY+1VVVsB+wGvB94ycYfpzow0sywoSdOSZFvgQ8BxVfWVqnqkqp6sqm9U1XuGPOe8JHclWZXk8iQvHBg7JMkNSR7qz37e3d8+N8k3kzyQ5L4k30uy3v93VdVPgO8Bew6csjs6ya3ApUk2SfK+JLckuTvJWf2vadCbk9zRnxm+eyDrPkmu6Ge6M8nHk2w+4bmHJFme5J4kH30qc5Kjknx/yPfns0n+LsmWwIXA9v3Z4MNJtk/yaJLtBvbfO8nKJJut7/sxjiwoSdO1LzAb+OoGPOdCYBfg2cAy4AsDY2cA/6uqtgb2BC7tbz8RuA2YR2+W9l5gve/RlmQP4GXAVQObfx/YHfgj4Kj+xyuA5wNbAR+f8DKv6Oc9EPg/Sfbvb18DvAuYS+/7sB9w7ITn/jGwCNib3ozyzevL/JSqegQ4GLijqrbqf9wBfAf404Fd3wCcU1VPTvW1x4kFJWm6tgPuqarVU31CVZ1ZVQ9V1ePAB4C9BmYtTwJ7JNmmqu6vqmUD258LPK8/Q/terftNRJcluR/4BnA68JmBsQ/0Z3q/Av4MOKWqllfVw8BfAodPOP33wf7+1/Zf54j+17G0qhZX1eqqWgF8il75DfpIVd1XVbfSOw16xFS/T+vwOeBIgP61tSOAzzfwup1kQUmarnuBuVO9npNkVpKTk/wsyYPAiv7Q3P5/XwMcAtyS5LtJ9u1v/yhwM3BR/5TZSes51N5VNaeqXlBV76uqtQNjvxh4vD1wy8DntwCb0pulTbb/Lf3nkGTX/mnHu/pfy4cHvo51Pvc39HV6Jb4TcACwqqp+2MDrdpIFJWm6rgAeB149xf1fT+9U1/7AtsDC/vYAVNWPquoweqf/vgZ8qb/9oao6saqeDxwKnJBkv2lmHpx53QE8b+DzBcBq4JcD2+ZPGL+j//hfgJ8Au1TVNvROO2bCsYY9dzpZexuqHqP3fTmS3um9jXb2BBaUpGmqqlXA3wCnJnl1ki2SbJbk4CR/P8lTtqZXaPcCW9CbdQCQZPP+7wdt27+e8iCwtj/2qiQ7Jwmwit71n7X/7dU33NnAu5LslGSrfp5zJ5yy/Ov+1/VC4E3AuQNfy4PAw0l+C3jbJK//niRzkswH3jnw3Kn6JbDdJAs3zqJ37exQLChJmlxVfQw4AXgfsJLeaa2305sBTXQWvVNdtwM3AIsnjL8BWNE/ZfZWeteIoLdI4T+Ah+nN2j5RVZc1EP9Mev+Dvxz4OfAY8I4J+3yX3unFS4B/qKqnfsH23fRmhA8Bn2by8vk6sBS4GvgWvUUgU9ZfhXg2sLy/WnD7/vYf0CvoZVV1y7peY9zFGxZK0nhJcinwxao6ve0so2RBSdIYSfI/gIuB+VX1UNt5RslTfJI0JpJ8jt7pzuM39nICZ1CSpI5a5+8vHLDJ62wvPe1dvPa8icuHJc0AT/FJkjrJd/SVWjR37txauHBh2zGkVi1duvSeqpo3cbsFJbVo4cKFLFmypO0YUquSTPr7XJ7ikyR1kgUlSeokC0qS1EkWlCSpkywoSVInWVCSpE6yoKQWXXv7Khae9K22Y0idZEFJkjrJgpIkdZIFJUnqJAtKaliSdya5Lsn1SY5vO480riwoqUFJ9gTeAuwD7AW8KsnO7aaSxpMFJTVrd+DKqnq0qlYD3wX+pOVM0liyoKRmXQe8LMl2SbYADgHmD+6Q5JgkS5IsWfPoqlZCSuPA221IDaqqG5N8BLgIeAS4GlgzYZ/TgNMAnvHcXbxrtTSEMyipYVV1RlX9TlW9HLgf+GnbmaRx5AxKaliSZ1fV3UkW0Lv+9JK2M0njyIKSmvdvSbYDngSOq6oH2g4kjSMLSmpYVb2s7QzSxsBrUJKkTrKgpBa9aIdtWXHyK9uOIXWSBSVJ6iQLSpLUSRaUJKmTLChJUidZUJKkTrKgJEmdZEFJDUvyrv7NCq9LcnaS2W1nksaRBSU1KMkOwF8Ai6pqT2AWcHi7qaTxZEFJzdsUeGaSTYEtgDtaziONJQtKalBV3Q78A3ArcCewqqouajeVNJ4sKKlBSeYAhwE7AdsDWyY5csI+v76j7sqVK9uIKY0FC0pq1v7Az6tqZVU9CXwFeOngDlV1WlUtqqpF8+bNayWkNA4sKKlZtwIvSbJFkgD7ATe2nEkaSxaU1KCquhL4MrAMuJbev7HTWg0ljSlvWCg1rKreD7y/7RzSuHMGJUnqJAtKktRJFpQkqZMsKElSJ1lQkqROchWf1KJrb1/FwpO+1XYMaYOtOPmVIz+GMyhJUidt1DOoR//4fw4du/O1TwwdO/elnxo69rrL3zp0rB6fNXRs9/cuHzq25p57h45ls82HjrHXrsPHZlgtvX7IQM1sEEkbDWdQkqROsqCkBiXZLcnVAx8PJjm+7VzSONqoT/FJM62qbgJ+GyDJLOB24KuthpLGlDMoaXT2A35WVbe0HUQaRxaUNDqHA2dP3Dh4w8I1j65qIZY0HiwoaQSSbA4cCpw3cWzwhoWztth25sNJY2J8rkFtMnwJ911/Mfly8s8ff8rQ57xwXcu31/Ft+a/9T1/H84Y75aW7DB274r7nDx3barPHh459ZsFZ08oyCrudc+zk2//5jqHPWb3i1lHF6YKDgWVV9cu2g0jjyhmUNBpHMMnpPUlTZ0FJDUuyJXAA8JW2s0jjbHxO8UljoqoeAbZrO4c07pxBSZI6yRmU1KIX7bAtS2bgXaGlceQMSpLUSWMzg3r8j/YeOrbsPR8fMrKupeQz64Q5/zV8cF1jY+Kmwz8x6fZ9rzlu6HPmbNzLzCX9hpxBSZI6yYKSJHWSBSVJ6iQLSpLUSRaU1LAkz0ry5SQ/SXJjkn3bziSNo7FZxSeNkX8C/r2qXtt/V/Mt2g4kjaNOFdSjfzL5u5IDnPV/P7aOZ3bj3/9q1gwdO/X+3YaOfeLbBw4dm72y+UnuD94+/Hu5VZ7R6LEe+K3hY3MaPVI3JNkWeDlwFEBVPQE80WYmaVx5ik9q1k7ASuAzSa5Kcnr/zWMlbSALSmrWpsDewL9U1YuBR4CTBncYvKPuypUr28gojQULSmrWbcBtVXVl//Mv0yusXxu8o+68efNmPKA0LiwoqUFVdRfwiyRPXXTcD7ihxUjS2OrUIglpI/EO4Av9FXzLgTe1nEcaSxaU1LCquhpY1HYOadx1qqDmHb986NiCTWduKflPn3xs6NghFx4/dGzBBcNfc/Y3fjh07AUsnlKuDTHrhcOXta89roY/MdM73p1rHp10+/z/cIW1pOnxGpQkqZMsKElSJ1lQkqROsqAkSZ1kQUmSOsmCkiR1UqeWma/64IKhYzu/5q2NHmu3Mx4ZOjbrngeHju16y/Dl4l0y/zO3Dh3bZpPZjR/vPx/bYdLtm16ytPFjSXp6cAYlSeqkTs2gpI1BkhXAQ8AaYHVV+a4S0jRYUNJovKKq7mk7hDTOPMUnSeokC0pqXgEXJVma5JiJg96wUJoaC0pq3u9V1d7AwcBxSV4+OOgNC6Wp6dQ1qHUtSd71kmaPtY7382Z1s4dqxfzZ98/o8d639NWTbt+JH89oji6oqtv7/707yVeBfYDL200ljR9nUFKDkmyZZOunHgMHAte1m0oaT52aQUkbgecAX00CvX9fX6yqf283kjSeLCipQVW1HNir7RzSxsBTfJKkTrKgJEmdZEFJkjrJa1Bj7N637Dt07I3P+ug6nrlF41nmXNj8a0p6enMGJUnqJAtKktRJFpQkqZMsKElSJ1lQkqROsqCkEUgyK8lVSb7ZdhZpXLnMvOM2nb/j0LGL/+ZjQ8e22WRml33POXfZpNvX9a7xG7l3AjcC27QdRBpXzqCkhiXZEXglcHrbWaRxZkFJzftH4H8Daycb9I660tRYUFKDkrwKuLuqht590zvqSlNjQUnN+l3g0CQrgHOAP0zyr+1GksaTBSU1qKr+sqp2rKqFwOHApVV1ZMuxpLFkQUmSOsll5h33xPPmDh3bZpPZM5gEFj++jsG1T+MF5UNU1XeA77QcQxpbzqAkSZ1kQUmSOsmCkiR1kgUlSeokC0qS1EkWlCSpk1xm3nGv+/RFM3q8Hzw+/GeWDx395qFjs56c/N3MJWm6nEFJkjrJgpIalGR2kh8m+XGS65N8sO1M0rjyFJ/UrMeBP6yqh5NsBnw/yYVVtbjtYNK4saCkBlVVAQ/3P92s/+H7QEnT4Ck+qWFJZiW5GrgbuLiqrmw7kzSOLCipYVW1pqp+G9gR2CfJnoPj3lFXmhpP8XXAyrftO3Ts6G1Obfx4q1kzdOxNXzt26NjOl3kZZUNU1QNJLgMOAq4b2H4acBrAokWLPP0nDeEMSmpQknlJntV//EzgAOAn7aaSxpMzKKlZzwU+l2QWvR8Av1RV32w5kzSWLCipQVV1DfDitnNIGwNP8UmSOsmCkiR1kgUlSeokr0F1wNrNMqPHe9H3jh46tvMJLiWX1A3OoCRJnWRBSZI6yYKSJHWSBSVJ6iQLSpLUSRaU1KAk85NcluSG/h1139l2Jmlcucx8hvzq1fsMHTvgqCtmMAnw8y1n9nhPL6uBE6tqWZKtgaVJLq6qG9oOJo0bZ1BSg6rqzqpa1n/8EHAjsEO7qaTxZEFJI5JkIb03jr1ywnZvWChNgQUljUCSrYB/A46vqgcHx6rqtKpaVFWL5s2b105AaQxYUFLDkmxGr5y+UFVfaTuPNK4sKKlBSQKcAdxYVae0nUcaZ67imyGrZw//WeDk5yydwSQasd8F3gBcm+Tq/rb3VtUFLWaSxpIFJTWoqr4PzOzb00sbKU/xSZI6yYKSJHWSBSVJ6iQLSpLUSRaUJKmTXMXXoFlz5gwd+6u/+2zzx8vwny8u+dWsoWM7ff2RxrNIUtOcQUmSOsmCkiR1kgUlNSjJmUnuTnJd21mkcWdBSc36LHBQ2yGkjYEFJTWoqi4H7ms7h7QxsKAkSZ3kMvMGrThu96FjBz3zksaPt6bWDh07dskbh44tXHxN41k0dUmOAY4BWLBgQctppO5yBiXNMO+oK02NBSVJ6iQLSmpQkrOBK4DdktyW5Oi2M0njymtQUoOq6oi2M0gbC2dQkqROsqAkSZ3kKb4GPTFn+LLvmTb/k/7RShpvzqAkSZ1kQUmSOsmCkiR1kgUlSeokC0qS1EkWlCSpk1yLPMb++YHnDx3b/LpfDB1bM4ow+rUkBwH/BMwCTq+qk1uOJI0lZ1BSg5LMAk4FDgb2AI5Iske7qaTxZEFJzdoHuLmqllfVE8A5wGEtZ5LGkgUlNWsHYPD86m39bb+W5JgkS5IsWbly5YyGk8aJBSXNMG9YKE2NBSU163Zg/sDnO/a3SdpAFpTUrB8BuyTZKcnmwOHA+S1nksaSy8w77sS79hk6tuTDvzN0bMuVV44ijtajqlYneTvwbXrLzM+squtbjiWNJQtKalhVXQBc0HYOadx5ik+S1EkWlCSpkywoSVInWVCSpE6yoCRJneQqvga94MTFQ8cOOXHvab7q6qEjW+JSckkbL2dQkqROsqAkSZ1kQUmSOsmCkiR1koskpBYtXbr04SQ3tZ1jwFzgnrZD9JllchtjludNttGCktp1U1UtajvEU5Is6Uoes0zu6ZRlnQV18drzMqoDS5K0Ll6DkiR1kgUlteu0tgNM0KU8Zpnc0yZLqmqUry9J0rQ4g5IkdZIFJc2AJAcluSnJzUlOmmT8GUnO7Y9fmWRhi1lOSHJDkmuSXJJk0iXAM5FlYL/XJKkkI129NpU8Sf60//25PskX28qSZEGSy5Jc1f+zOmREOc5McneS64aMJ8n/6+e8Jsl033j0v6sqP/zwY4QfwCzgZ8Dzgc2BHwN7TNjnWOCT/ceHA+e2mOUVwBb9x29rM0t/v62By4HFwKKW/5x2Aa4C5vQ/f3aLWU4D3tZ/vAewYkRZXg7sDVw3ZPwQ4EIgwEuAK5s6tjMoafT2AW6uquVV9QRwDnDYhH0OAz7Xf/xlYL8ko/g1j/VmqarLqurR/qeLgR1HkGNKWfr+FvgI8NiIcmxInrcAp1bV/QBVdXeLWQrYpv94W+COUQSpqsuB+9axy2HAWdWzGHhWkuc2cWwLShq9HYBfDHx+W3/bpPtU1WpgFbBdS1kGHU3vp+NRWG+W/umi+VX1rRFl2KA8wK7Arkl+kGRxkoNazPIB4MgktwEXAO8YUZb12dC/U1PmO0lImlSSI4FFwO+3dPxNgFOAo9o4/hCb0jvN9wf0ZpaXJ3lRVT3QQpYjgM9W1ceS7At8PsmeVbW2hSwj4QxKGr3bgfkDn+/Y3zbpPkk2pXfK5t6WspBkf+CvgEOr6vER5JhKlq2BPYHvJFlB7/rG+SNcKDGV781twPlV9WRV/Rz4Kb3CaiPL0cCXAKrqCmA2vffGm2lT+js1HRaUNHo/AnZJslOSzektgjh/wj7nA3/ef/xa4NLqX4Ge6SxJXgx8il45jeoay3qzVNWqqppbVQuraiG962GHVtWSNvL0fY3e7Ikkc+md8lveUpZbgf36WXanV1ArR5Blfc4H3thfzfcSYFVV3dnEC3uKTxqxqlqd5O3At+mtzjqzqq5P8iFgSVWdD5xB7xTNzfQuSB/eYpaPAlsB5/XXadxaVYe2lGXGTDHPt4EDk9wArAHeU1WNz3SnmOVE4NNJ3kVvwcRRo/ihJsnZ9Ep5bv961/uBzfo5P0nv+tchwM3Ao8CbGjv2aH5IkyTpN+MpPklSJ1lQkqROsqAkSZ1kQUmSOsmCkiR1kgUlSeokC0qS1EkWlCSpk/4/pctlbYVqyfEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x648 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXYAPkb_LMLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}